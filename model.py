
import torch
from torch import nn
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
# åªå¯¼å…¥éœ€è¦çš„transformersæ¨¡å—ï¼Œé¿å…TensorFlowå†²çª
from transformers import BertTokenizer, BertForSequenceClassification
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

class StockLSTMRegressor(nn.Module):
    def __init__(self, input_size=2, hidden_size=64, num_layers=2, output_size=1, dropout=0.2):
        """
        LSTMæ¨¡å‹ç”¨äºè‚¡ä»·é¢„æµ‹
        Args:
            input_size: è¾“å…¥ç‰¹å¾æ•°é‡ï¼ˆæ¶¨å¹… + æƒ…æ„ŸæŒ‡æ ‡ = 2ï¼‰
            hidden_size: LSTMéšè—å±‚å¤§å°
            num_layers: LSTMå±‚æ•°
            output_size: è¾“å‡ºå¤§å°ï¼ˆ1è¡¨ç¤ºé¢„æµ‹ä¸‹ä¸ªæœˆçš„æ¶¨å¹…ï¼‰
            dropout: dropoutç‡
        """
        super(StockLSTMRegressor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                           dropout=dropout if num_layers > 1 else 0,
                           batch_first=True)

        # Dropout layer
        self.dropout = nn.Dropout(dropout)

        # Linear layer for final prediction
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x shape: (batch_size, sequence_length, input_size)
        # sequence_length = 4 (è¿‡å»4ä¸ªæœˆ)
        # input_size = 2 (æ¶¨å¹… + æƒ…æ„ŸæŒ‡æ ‡)

        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)

        # LSTM forward pass
        lstm_out, _ = self.lstm(x, (h0, c0))

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]

        # Dropout
        out = self.dropout(last_output)

        # Linear layer
        out = self.linear(out)

        return out

def calculate_sentiment_score(headlines, finbert_model, tokenizer):
    # å‚æ•°éªŒè¯
    if finbert_model is None:
        print("âŒ FinBERTæ¨¡å‹ä¸ºNone")
        return 0.0
    
    if tokenizer is None:
        print("âŒ Tokenizerä¸ºNone")
        return 0.0

    if not headlines or len(headlines) == 0:
        print("âš ï¸ æ²¡æœ‰æ–°é—»æ ‡é¢˜ç”¨äºæƒ…æ„Ÿåˆ†æ")
        return 0.0

    sentiment_scores = []
    
    try:
        device = next(finbert_model.parameters()).device
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹è®¾å¤‡å¤±è´¥: {e}")
        device = torch.device("cpu")
        
    print(f"ğŸ“ æ­£åœ¨åˆ†æ {len(headlines)} ä¸ªæ–°é—»æ ‡é¢˜çš„æƒ…æ„Ÿ")

    for headline in headlines:
        try:
            # ä½¿ç”¨FinBERTè®¡ç®—æƒ…æ„Ÿå¾—åˆ†
            encoded = tokenizer.encode_plus(
                headline,
                add_special_tokens=True,
                max_length=64,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
            input_ids = encoded['input_ids'].to(device)
            attention_mask = encoded['attention_mask'].to(device)

            with torch.no_grad():
                outputs = finbert_model(input_ids=input_ids, attention_mask=attention_mask)

                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1).cpu().numpy()[0]

                # FinBERTè¾“å‡º: [negative, neutral, positive]
                # è½¬æ¢ä¸ºè¿ç»­çš„æƒ…æ„Ÿå¾—åˆ†: positive - negative
                weighted_score = probabilities[0] - probabilities[1]
                sentiment_scores.append(weighted_score)

        except Exception as e:
            print(f"å¤„ç†æ ‡é¢˜æ—¶å‡ºé”™: {headline[:50]}... é”™è¯¯: {e}")
            # å‡ºé”™æ—¶ä½¿ç”¨ä¸­æ€§å¾—åˆ†
            sentiment_scores.append(0.0)

    return float(np.mean(sentiment_scores)) if sentiment_scores else 0.0
